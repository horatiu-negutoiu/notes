Hadoop
	HDFS
	Map/Reduce

NameNode
- maintains and manages DataNodes
- Records metadata about data blocks (location of blocks, size, permissions)
- receives hearbeat and block report from all the DataNodes

DataNode
- Slave daemons
- Stores actual data
- Serves read and write requests

Secondary NameNode
- Checkpointing is a process of combining Edit Logs and FsImage
- Secondary NameNode takes over the responsiblity of checkpointing, therefore, making NameNode more available
- Allows faster Failover as it prevents edit logs from getting too large
- Checkpointing happens periodically (1 hour)

          NameNode  <--------->  Secondary NameNode
		  /   |   \                    ...
		 /    |    \
  DataNode DataNode DataNode

FsImage
- contains the changes that have happened
- on disk

Edit log
- contains recent modifications
- in memory 

HDFS data blocks at 128MB

Fault Tolerance: Replication Factor
Each data block replicated three times and distributed across different DataNodes

HDFS Write Mechanism
1. Pipeline Setup
	A) Write pipeline
		1. Write request Block A to NameNode
		2. NameNode returns 3 DataNodes
		3. Asks DN1 if ready then
			Ask DN2 if ready then
				Ask DN3 if ready
	B) Actual writing
		1. Write request
		2. Request DN1 to write then
			Request DN2 to write then
				Request DN3 to write
	C) Acknowledgment Write
		1. DN3 ack to DN2 success
		2. DN2 ack to DN1 success
		3. DN1 ack to NameNode

- multiple blocks are copied at the same time
		
HDFS Read Mechanism
1. Read request
2. NameNode returns 3 DataNodes
3. Blocks are read from DataNodes

Map/Reduce
- Map: everyone doing a little bit of work
- Reduce: adding the intermediate results together
1. Mapper Code: how mapping tasks will happen at th same time and how they will generate the key/value pairs
2. Reducer Code: Combine intermediate key/value pairs generated by Mapper
3. Driver Code: Specify job configuration













